{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNJao9uShdcI8WpGIUXQtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielBugelnig/U-Net/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ji5UAnpqcEB7",
        "outputId": "944c5e57-3d8f-412a-b142-840661df6241",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cujGZPN6cmx4",
        "outputId": "77fceda6-1fab-4851-b1cf-635a5c07d7bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pytorch libraries\n",
        "import torch\n",
        "import torch.nn.functional as F #for ReLu\n",
        "import torchvision.transforms.functional as Trans\n",
        "from torchinfo import summary\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter, map_coordinates"
      ],
      "metadata": {
        "id": "rqhegTAgcpZR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(module):\n",
        "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d):\n",
        "        #Apply He for ReLU\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_number, output_number):\n",
        "        super(UNet, self).__init__()\n",
        "        self.input_number = input_number\n",
        "        self.output_number = output_number\n",
        "\n",
        "        #  Initialize weights\n",
        "        self.apply(init_weights)\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        # input: 4d tensor: batch_size x input_number x 572x572 --> input number=1 for grayscale image, 3 for RGB image\n",
        "        # assuming 572x572 image\n",
        "        self.conv1 = nn.Conv2d(self.input_number, 64, kernel_size=3, padding=0) # 64x570x570\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # 64x568x568\n",
        "        self.maxPool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 64x284x284\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # 128x282x282\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # 128x280x280\n",
        "        self.maxPool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 128x140x140\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128,256, kernel_size=3, padding=0) # 256x138x138\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # 256x136x136\n",
        "        self.maxPool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 256x68x68\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # 512x66x66\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # 512x64x64\n",
        "        self.maxPool4 = nn.MaxPool2d(kernel_size=2, stride=2) # 512x32x32\n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # 1024x30x30\n",
        "        self.conv10 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # 1024x28x28\n",
        "\n",
        "\n",
        "        # Between Decoder and Encoder one dropout connection is added\n",
        "        # Dropout layer\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Decoder\n",
        "        # Upsampling by a factor of 2, --> stride=2, kernel_size=2\n",
        "        # 2x2 up convolution halves the feature channels\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 512x56x56 --> output size formular(input) # stride * (input-1) + kernel_size -2 *padding + output_padding = 2*(28-1) + 2 - 2*0 + 0 = 56\n",
        "        self.conv1b = nn.Conv2d(1024, 512, kernel_size=3, padding=0) # 512x54x54 other 512 features come from encoder site\n",
        "        self.conv2b = nn.Conv2d(512, 512, kernel_size=3, padding=0) # 512x52x52\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, 2, 2) # 512x104x104\n",
        "        self.conv3b = nn.Conv2d(512, 256, 3, padding=0) #256x102x102\n",
        "        self.conv4b = nn.Conv2d(256, 256, 3, padding=0) # 256x100x100\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2,2,) #256x200x200\n",
        "        self.conv5b = nn.Conv2d(256, 128, 3, padding=0) #128x198x198\n",
        "        self.conv6b = nn.Conv2d(128, 128, 3, padding=0) #128x196x196\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, 2, 2) #128x392x392\n",
        "        self.conv7b = nn.Conv2d(128, 64, 3, padding=0) #64x390x390\n",
        "        self.conv8b = nn.Conv2d(64,64,3, padding=0) # 64x388x388\n",
        "        self.final_conv = nn.Conv2d(64, self.output_number, kernel_size=1, padding=0) #2x388x388\n",
        "\n",
        "    def cropConcat(self, encoder, decoder):\n",
        "        # crops the encoder tensor and concatenate its with the decoder tensor\n",
        "        _,_,H,W = decoder.shape\n",
        "        cropped_enc = Trans.center_crop(encoder, [H,W]) # crops the encoder tensor in the centre\n",
        "        return torch.cat((cropped_enc, decoder), dim=1) # concatenates at the feature dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x1 = F.relu(self.conv2(x))\n",
        "        x = self.maxPool1(x1)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x2 = F.relu(self.conv4(x))\n",
        "        x = self.maxPool2(x2)\n",
        "\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x3 = F.relu(self.conv6(x))\n",
        "        x = self.maxPool3(x3)\n",
        "\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x4 = F.relu(self.conv8(x))\n",
        "        x = self.maxPool4(x4)\n",
        "\n",
        "        x = F.relu(self.conv9(x))\n",
        "        x = F.relu(self.conv10(x))\n",
        "\n",
        "        # Dropout\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.upconv1(x)  # size 512x56x56\n",
        "\n",
        "        x = self.cropConcat(x4, x) # concatination1 size 1024x56x56\n",
        "        x = F.relu(self.conv1b(x))\n",
        "        x = F.relu(self.conv2b(x))\n",
        "        x = self.upconv2(x)\n",
        "\n",
        "        x = self.cropConcat(x3,x)\n",
        "        x = F.relu(self.conv3b(x))\n",
        "        x = F.relu(self.conv4b(x))\n",
        "        x = self.upconv3(x)\n",
        "\n",
        "        x = self.cropConcat(x2,x)\n",
        "        x = F.relu(self.conv5b(x))\n",
        "        x = F.relu(self.conv6b(x))\n",
        "        x = self.upconv4(x)\n",
        "\n",
        "        x = self.cropConcat(x1,x)\n",
        "        x = F.relu(self.conv7b(x))\n",
        "        x = F.relu(self.conv8b(x))\n",
        "        x = self.final_conv(x)\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MQdWJB23cr9R"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(1,1)\n",
        "summary(model, input_size=(1, 1, 572, 572))  # Example input size"
      ],
      "metadata": {
        "id": "HUQeiCvEcy9y",
        "outputId": "48bae8df-fa6b-4aa6-f934-3592c02e012d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "UNet                                     [1, 1, 388, 388]          --\n",
              "├─Conv2d: 1-1                            [1, 64, 570, 570]         640\n",
              "├─Conv2d: 1-2                            [1, 64, 568, 568]         36,928\n",
              "├─MaxPool2d: 1-3                         [1, 64, 284, 284]         --\n",
              "├─Conv2d: 1-4                            [1, 128, 282, 282]        73,856\n",
              "├─Conv2d: 1-5                            [1, 128, 280, 280]        147,584\n",
              "├─MaxPool2d: 1-6                         [1, 128, 140, 140]        --\n",
              "├─Conv2d: 1-7                            [1, 256, 138, 138]        295,168\n",
              "├─Conv2d: 1-8                            [1, 256, 136, 136]        590,080\n",
              "├─MaxPool2d: 1-9                         [1, 256, 68, 68]          --\n",
              "├─Conv2d: 1-10                           [1, 512, 66, 66]          1,180,160\n",
              "├─Conv2d: 1-11                           [1, 512, 64, 64]          2,359,808\n",
              "├─MaxPool2d: 1-12                        [1, 512, 32, 32]          --\n",
              "├─Conv2d: 1-13                           [1, 1024, 30, 30]         4,719,616\n",
              "├─Conv2d: 1-14                           [1, 1024, 28, 28]         9,438,208\n",
              "├─Dropout: 1-15                          [1, 1024, 28, 28]         --\n",
              "├─ConvTranspose2d: 1-16                  [1, 512, 56, 56]          2,097,664\n",
              "├─Conv2d: 1-17                           [1, 512, 54, 54]          4,719,104\n",
              "├─Conv2d: 1-18                           [1, 512, 52, 52]          2,359,808\n",
              "├─ConvTranspose2d: 1-19                  [1, 256, 104, 104]        524,544\n",
              "├─Conv2d: 1-20                           [1, 256, 102, 102]        1,179,904\n",
              "├─Conv2d: 1-21                           [1, 256, 100, 100]        590,080\n",
              "├─ConvTranspose2d: 1-22                  [1, 128, 200, 200]        131,200\n",
              "├─Conv2d: 1-23                           [1, 128, 198, 198]        295,040\n",
              "├─Conv2d: 1-24                           [1, 128, 196, 196]        147,584\n",
              "├─ConvTranspose2d: 1-25                  [1, 64, 392, 392]         32,832\n",
              "├─Conv2d: 1-26                           [1, 64, 390, 390]         73,792\n",
              "├─Conv2d: 1-27                           [1, 64, 388, 388]         36,928\n",
              "├─Conv2d: 1-28                           [1, 1, 388, 388]          65\n",
              "==========================================================================================\n",
              "Total params: 31,030,593\n",
              "Trainable params: 31,030,593\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 167.45\n",
              "==========================================================================================\n",
              "Input size (MB): 1.31\n",
              "Forward/backward pass size (MB): 1073.62\n",
              "Params size (MB): 124.12\n",
              "Estimated Total Size (MB): 1199.05\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform=False):\n",
        "        self.images = Image.open(image_path)\n",
        "        self.labels = Image.open(label_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.n_frames\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Access specific frame\n",
        "        self.images.seek(idx)\n",
        "        self.labels.seek(idx)\n",
        "\n",
        "\n",
        "\n",
        "        # Convert to grayscale\n",
        "        image = self.images.convert(\"L\")\n",
        "        label = self.labels.convert(\"L\")\n",
        "\n",
        "        if transforms:\n",
        "\n",
        "          # Random horizontal flipping\n",
        "          if random.random() > 0.5:\n",
        "              image = Trans.hflip(image)\n",
        "              label = Trans.hflip(label)\n",
        "\n",
        "        # Random vertical flipping\n",
        "          if random.random() > 0.5:\n",
        "              image = Trans.vflip(image)\n",
        "              label = Trans.vflip(label)\n",
        "\n",
        "        # Transform to tensor\n",
        "        image = Trans.to_tensor(image)\n",
        "        label = Trans.to_tensor(label)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "t2orgm-Fc0th"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCP7Z8h_32Jr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = 0.5\n",
        "std = 0.2\n",
        "\n",
        "# Data transforms\n",
        "basic_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "basic_transform_resize = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([572, 572])\n",
        "])\n",
        "\n",
        "# do not work, the transform are applied directly in the dataset class\n",
        "advanced_transform_image = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    #transforms.RandomRotation(degrees=15),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[mean], std=[std])\n",
        "])\n",
        "\n",
        "advanced_transform_label = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    #transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "KkRuXJaEdZpW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeMeanStd(dataloader):\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for images, _ in dataloader:\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, -1)\n",
        "        mean += images.mean(1).sum(0)\n",
        "        std += images.std(1).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "\n",
        "    return mean.item(), std.item()"
      ],
      "metadata": {
        "id": "cSedk8HddczS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # for google colab, vanilla dataset ISBI 2012\n",
        "train_image_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-challenge/train-volume.tif\"\n",
        "train_label_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-challenge/train-labels.tif\"\n",
        "test_image_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-challenge/test-volume.tif\"\n",
        "test_label_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-challenge/test-labels.tif\"\n",
        "\n",
        "# for google colab, mirrored dataset ISBI 2012\n",
        "train_image_path = \"/content/drive/MyDrive/Machine_Learning/ISBI-2012-mirrored/train-mirror.tif\"\n",
        "train_label_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-mirrored/train-labels.tif\"\n",
        "test_image_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-mirrored/test-mirror.tif\"\n",
        "test_label_path = \"/content/drive/My Drive/Machine_Learning/ISBI-2012-mirrored/test-labels.tif\"\n"
      ],
      "metadata": {
        "id": "_ddQ7Dd5dr4E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, nrOfEpochs):\n",
        "  model.train()\n",
        "  for epoch in range(nrOfEpochs):\n",
        "      running_loss = 0.0\n",
        "      for images, labels in dataloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          labels = Trans.center_crop(labels, [388, 388])\n",
        "          #images_vis = Trans.center_crop(images, [388, 388])\n",
        "          #visualize_sample(images_vis[0], labels[0])\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      train_loss = running_loss / len(dataloader)\n",
        "      print(f\"Epoch {epoch + 1}/{nrOfEpochs}, Loss: {train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "H51isxzLd-C8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    total_TP = total_TN = total_FP = total_FN = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            outputs_bin = (outputs > 0.5).int()\n",
        "            labels = Trans.center_crop(labels, [388, 388])\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            total_TP += ((outputs_bin == 1) & (labels == 1)).sum().item()\n",
        "            total_TN += ((outputs_bin == 0) & (labels == 0)).sum().item()\n",
        "            total_FP += ((outputs_bin == 1) & (labels == 0)).sum().item()\n",
        "            total_FN += ((outputs_bin == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "    test_loss = running_loss / len(dataloader)\n",
        "    accuracy = (total_TP + total_TN) / (total_TP + total_TN + total_FP + total_FN)\n",
        "    precision = total_TP / (total_TP + total_FP)\n",
        "    recall = total_TP / (total_TP + total_FN)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    print(f\"Test loss: {test_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "UCMqijwXeEE0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Initial transform for computing mean and std\n",
        "computing_mean_std_dataset = Dataset(train_image_path, train_label_path, transform=False)\n",
        "computing_mean_std_dataset_loader = DataLoader(computing_mean_std_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "mean, std = computeMeanStd(computing_mean_std_dataset_loader)\n",
        "\n",
        "print(f\"Mean: {mean}, Std: {std}\")\n",
        "\n",
        "\n",
        "# Datasets and loaders\n",
        "# Vanilla Dataset: use basic_transform_resize\n",
        "# Mirrored Dataset: use advanced_transform_image and advanced_transform_label or : basic_transform for both\n",
        "train_dataset = Dataset(train_image_path, train_label_path, transform = True)\n",
        "test_dataset = Dataset(test_image_path, test_label_path, transform=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(1, 1).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "TkpT2K3oeJRF",
        "outputId": "848343de-f65c-474e-a7e7-22e4cc354df3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0.49460574984550476, Std: 0.16874875128269196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(image, label):\n",
        "    # Convert tensors to numpy arrays for visualization\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().numpy()\n",
        "        image = np.squeeze(image)  # Remove channel dimension if grayscale\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "        label = np.squeeze(label)  # Remove channel dimension if necessary\n",
        "\n",
        "    # Plot the image and label side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Display image\n",
        "    ax[0].imshow(image, cmap='gray')\n",
        "    ax[0].set_title('Transformed Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # Display label\n",
        "    ax[1].imshow(label, cmap='gray')\n",
        "    ax[1].set_title('Transformed Label')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fZX8DPL4v4dU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing CUDA memory\n",
        "torch.cuda.empty_cache()\n",
        "# Training\n",
        "\n",
        "print(\"Starting training\")\n",
        "train(model, train_loader, criterion, optimizer, nrOfEpochs=120)"
      ],
      "metadata": {
        "id": "q2-Lz3d3eSR_",
        "outputId": "687450cc-98b7-477a-bf49-a542e043d4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training\n",
            "Epoch 1/120, Loss: 0.5424\n",
            "Epoch 2/120, Loss: 0.4164\n",
            "Epoch 3/120, Loss: 0.3644\n",
            "Epoch 4/120, Loss: 0.3563\n",
            "Epoch 5/120, Loss: 0.3513\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b876f75c5d9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrOfEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-1c6bb6d5c7da>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, nrOfEpochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "print(\"Starting evaluation\")\n",
        "test(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "NwJtbwVEeZbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/My Drive/Machine_Learning/results/model_120ep_dropout10_mirror_transf_normalized.pth\")\n",
        "print(\"Model saved\")"
      ],
      "metadata": {
        "id": "wFwSrWDFebJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}