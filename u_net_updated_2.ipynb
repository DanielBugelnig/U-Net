{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxIwiW3rl1NznjtWBMJliE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielBugelnig/U-Net/blob/arthur/u_net_updated_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "hxsZZc-gpiEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46daa5de-d3cd-49c0-e819-ac8d3dc2e13f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating U-Net architecture\n",
        "\n",
        "# page 4 of the paper: (https://arxiv.org/pdf/1505.04597.pdf)\n",
        "\n",
        "# Network Architecture\n",
        "# The network architecture is illustrated in Figure 1. It consists of a contracting\n",
        "# path (left side) and an expansive path (right side). The contracting path follows\n",
        "# the typical architecture of a convolutional network. It consists of the repeated\n",
        "# application of two 3x3 convolutions (unpadded convolutions), each followed by\n",
        "# a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2\n",
        "# for downsampling. At each downsampling step we double the number of feature\n",
        "# channels. Every step in the expansive path consists of an upsampling of the\n",
        "# feature map followed by a 2x2 convolution (“up-convolution”) that halves the\n",
        "# number of feature channels, a concatenation with the correspondingly cropped\n",
        "# feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in\n",
        "# every convolution. At the final layer a 1x1 convolution is used to map each 64-\n",
        "# component feature vector to the desired number of classes. In total the network\n",
        "# has 23 convolutional layers.\n",
        "# To allow a seamless tiling of the output segmentation map (see Figure 2), it\n",
        "# is important to select the input tile size such that all 2x2 max-pooling operations\n",
        "# are applied to a layer with an even x- and y-size.\n",
        "\n",
        "# The reduced output size within a single tile (e.g., 388x388 for a 572x572 input) ensures that the predictions are based on full context,\n",
        "# avoiding incomplete or invalid segmentations near the borders.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #for ReLu\n",
        "import torchvision.transforms.functional as Trans\n",
        "from torchinfo import summary\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_number, output_number):\n",
        "        super(UNet, self).__init__()\n",
        "        self.input_number = input_number\n",
        "        self.output_number = output_number\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        # input: 4d tensor: batch_size x input_number x 572x572 --> input number=1 for grayscale image, 3 for RGB image\n",
        "        # assuming 572x572 image\n",
        "        self.conv1 = nn.Conv2d(self.input_number, 64, kernel_size=3, padding=0) # 64x570x570\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # 64x568x568\n",
        "        self.maxPool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 64x284x284\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # 128x282x282\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # 128x280x280\n",
        "        self.maxPool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 128x140x140\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128,256, kernel_size=3, padding=0) # 256x138x138\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # 256x136x136\n",
        "        self.maxPool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 256x68x68\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # 512x66x66\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # 512x64x64\n",
        "        self.maxPool4 = nn.MaxPool2d(kernel_size=2, stride=2) # 512x32x32\n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # 1024x30x30\n",
        "        self.conv10 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # 1024x28x28\n",
        "\n",
        "        # Decoder\n",
        "        # Upsampling by a factor of 2, --> stride=2, kernel_size=2\n",
        "        # 2x2 up convolution halves the feature channels\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 512x56x56 --> output size formular(input) # stride * (input-1) + kernel_size -2 *padding + output_padding = 2*(28-1) + 2 - 2*0 + 0 = 56\n",
        "        self.conv1b = nn.Conv2d(1024, 512, kernel_size=3, padding=0) # 512x54x54 other 512 features come from encoder site\n",
        "        self.conv2b = nn.Conv2d(512, 512, kernel_size=3, padding=0) # 512x52x52\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, 2, 2) # 512x104x104\n",
        "        self.conv3b = nn.Conv2d(512, 256, 3, padding=0) #256x102x102\n",
        "        self.conv4b = nn.Conv2d(256, 256, 3, padding=0) # 256x100x100\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2,2,) #256x200x200\n",
        "        self.conv5b = nn.Conv2d(256, 128, 3, padding=0) #128x198x198\n",
        "        self.conv6b = nn.Conv2d(128, 128, 3, padding=0) #128x196x196\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, 2, 2) #128x392x392\n",
        "        self.conv7b = nn.Conv2d(128, 64, 3, padding=0) #64x390x390\n",
        "        self.conv8b = nn.Conv2d(64,64,3, padding=0) # 64x388x388\n",
        "        self.final_conv = nn.Conv2d(64, self.output_number, kernel_size=1, padding=0) #2x388x388\n",
        "\n",
        "    def cropConcat(self, encoder, decoder):\n",
        "        # crops the encoder tensor and concatenate its with the decoder tensor\n",
        "        _,_,H,W = decoder.shape\n",
        "        cropped_enc = Trans.center_crop(encoder, [H,W]) # crops the encoder tensor in the centre\n",
        "        return torch.cat((cropped_enc, decoder), dim=1) # concatenates at the feature dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x1 = F.relu(self.conv2(x))\n",
        "        x = self.maxPool1(x1)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x2 = F.relu(self.conv4(x))\n",
        "        x = self.maxPool2(x2)\n",
        "\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x3 = F.relu(self.conv6(x))\n",
        "        x = self.maxPool3(x3)\n",
        "\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x4 = F.relu(self.conv8(x))\n",
        "        x = self.maxPool4(x4)\n",
        "\n",
        "        x = F.relu(self.conv9(x))\n",
        "        x = F.relu(self.conv10(x))\n",
        "\n",
        "        # Decoder\n",
        "        x = self.upconv1(x)  # size 512x56x56\n",
        "\n",
        "        x = self.cropConcat(x4, x) # concatination1 size 1024x56x56\n",
        "        x = F.relu(self.conv1b(x))\n",
        "        x = F.relu(self.conv2b(x))\n",
        "        x = self.upconv2(x)\n",
        "\n",
        "        x = self.cropConcat(x3,x)\n",
        "        x = F.relu(self.conv3b(x))\n",
        "        x = F.relu(self.conv4b(x))\n",
        "        x = self.upconv3(x)\n",
        "\n",
        "        x = self.cropConcat(x2,x)\n",
        "        x = F.relu(self.conv5b(x))\n",
        "        x = F.relu(self.conv6b(x))\n",
        "        x = self.upconv4(x)\n",
        "\n",
        "        x = self.cropConcat(x1,x)\n",
        "        x = F.relu(self.conv7b(x))\n",
        "        x = F.relu(self.conv8b(x))\n",
        "        x = self.final_conv(x)\n",
        "        #x = F.softmax(x,1)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = UNet(1,1)\n",
        "summary(model, input_size=(1, 1, 572, 572))  # Example input size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1zE0_aCpVND",
        "outputId": "7ca5a181-676a-41b7-f5f6-1b2d92a69b2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "UNet                                     [1, 1, 388, 388]          --\n",
              "├─Conv2d: 1-1                            [1, 64, 570, 570]         640\n",
              "├─Conv2d: 1-2                            [1, 64, 568, 568]         36,928\n",
              "├─MaxPool2d: 1-3                         [1, 64, 284, 284]         --\n",
              "├─Conv2d: 1-4                            [1, 128, 282, 282]        73,856\n",
              "├─Conv2d: 1-5                            [1, 128, 280, 280]        147,584\n",
              "├─MaxPool2d: 1-6                         [1, 128, 140, 140]        --\n",
              "├─Conv2d: 1-7                            [1, 256, 138, 138]        295,168\n",
              "├─Conv2d: 1-8                            [1, 256, 136, 136]        590,080\n",
              "├─MaxPool2d: 1-9                         [1, 256, 68, 68]          --\n",
              "├─Conv2d: 1-10                           [1, 512, 66, 66]          1,180,160\n",
              "├─Conv2d: 1-11                           [1, 512, 64, 64]          2,359,808\n",
              "├─MaxPool2d: 1-12                        [1, 512, 32, 32]          --\n",
              "├─Conv2d: 1-13                           [1, 1024, 30, 30]         4,719,616\n",
              "├─Conv2d: 1-14                           [1, 1024, 28, 28]         9,438,208\n",
              "├─ConvTranspose2d: 1-15                  [1, 512, 56, 56]          2,097,664\n",
              "├─Conv2d: 1-16                           [1, 512, 54, 54]          4,719,104\n",
              "├─Conv2d: 1-17                           [1, 512, 52, 52]          2,359,808\n",
              "├─ConvTranspose2d: 1-18                  [1, 256, 104, 104]        524,544\n",
              "├─Conv2d: 1-19                           [1, 256, 102, 102]        1,179,904\n",
              "├─Conv2d: 1-20                           [1, 256, 100, 100]        590,080\n",
              "├─ConvTranspose2d: 1-21                  [1, 128, 200, 200]        131,200\n",
              "├─Conv2d: 1-22                           [1, 128, 198, 198]        295,040\n",
              "├─Conv2d: 1-23                           [1, 128, 196, 196]        147,584\n",
              "├─ConvTranspose2d: 1-24                  [1, 64, 392, 392]         32,832\n",
              "├─Conv2d: 1-25                           [1, 64, 390, 390]         73,792\n",
              "├─Conv2d: 1-26                           [1, 64, 388, 388]         36,928\n",
              "├─Conv2d: 1-27                           [1, 1, 388, 388]          65\n",
              "==========================================================================================\n",
              "Total params: 31,030,593\n",
              "Trainable params: 31,030,593\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 167.45\n",
              "==========================================================================================\n",
              "Input size (MB): 1.31\n",
              "Forward/backward pass size (MB): 1073.62\n",
              "Params size (MB): 124.12\n",
              "Estimated Total Size (MB): 1199.05\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as Trans\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "'''\n",
        "#Loading data\n",
        "#https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "class Dataset(Dataset):\n",
        "  def __init__(self, image_path, label_path, transform=None):\n",
        "    self.images = Image.open(image_path)\n",
        "    self.labels = Image.open(label_path)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.images.n_frames\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #find specific frame\n",
        "    self.images.seek(idx)\n",
        "    self.labels.seek(idx)\n",
        "    #grayscale conversion (if necessary)\n",
        "    image = self.images.convert(\"L\")\n",
        "    label = self.labels.convert(\"L\")\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "      label = self.transform(label)\n",
        "    return image, label\n",
        "'''\n",
        "#Transforming dataset from Daniel\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform=True):\n",
        "        self.images = Image.open(image_path)\n",
        "        self.labels = Image.open(label_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.n_frames\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Access specific frame\n",
        "        self.images.seek(idx)\n",
        "        self.labels.seek(idx)\n",
        "\n",
        "        # Convert to grayscale\n",
        "        image = self.images.convert(\"L\")\n",
        "        label = self.labels.convert(\"L\")\n",
        "\n",
        "        if transforms:\n",
        "\n",
        "          # Random horizontal flipping\n",
        "          if random.random() > 0.5:\n",
        "              image = Trans.hflip(image)\n",
        "              label = Trans.hflip(label)\n",
        "\n",
        "        # Random vertical flipping\n",
        "          if random.random() > 0.5:\n",
        "              image = Trans.vflip(image)\n",
        "              label = Trans.vflip(label)\n",
        "\n",
        "        # Transform to tensor\n",
        "        image = Trans.to_tensor(image)\n",
        "        label = Trans.to_tensor(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "#original\n",
        "#train_dataset = Dataset(\"ISBI-2012-challenge/train-volume.tif\", \"ISBI-2012-challenge/train-labels.tif\", transform)\n",
        "#test_dataset = Dataset(\"ISBI-2012-challenge/test-volume.tif\", \"ISBI-2012-challenge/test-labels.tif\", transform)\n",
        "\n",
        "#mirrored\n",
        "train_dataset = Dataset(\"ISBI-2012-challenge/train-mirror.tif\", \"ISBI-2012-challenge/train-labels.tif\", transform)\n",
        "test_dataset = Dataset(\"ISBI-2012-challenge/test-mirror.tif\", \"ISBI-2012-challenge/test-labels.tif\", transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "#print(train_dataset.images.n_frames)\n",
        "\n",
        "#print(train_dataset.shape)\n",
        "\n",
        "#Training - pick optimizer and learning rate\n",
        "#https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(1,1).to(device)\n",
        "#optimizer= optim.Adam(model.parameters(),0.001)\n",
        "optimizer = optim.SGD(model.parameters(),0.0001,momentum=0.99)\n",
        "#criterion = nn.CrossEntropyLoss() #for some reason doesn't work\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, nrOfEpochs):\n",
        "  model.train()\n",
        "  for i in range(nrOfEpochs):\n",
        "    running_loss = 0.0\n",
        "    avgAcc=0.0\n",
        "    avgPrec=0.0\n",
        "    avgRec=0.0\n",
        "    avgF1=0.0\n",
        "    for images, labels in dataloader:\n",
        "      images, labels = images.to(device), labels.to(device);\n",
        "      #print(images.shape)\n",
        "      optimizer.zero_grad()\n",
        "      outputs=model(images)\n",
        "      #outputs = (outputs > 0.5).float()\n",
        "      labels = Trans.center_crop(labels, [388,388])\n",
        "      #assert outputs.shape == labels.shape, f\"Shape mismatch: {outputs.shape} vs {labels.shape}\"\n",
        "      loss = criterion(outputs,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      if(i==nrOfEpochs-1):\n",
        "        for j in range(dataloader.batch_size):\n",
        "          outputs = (outputs > 0.5).int()\n",
        "          labels = labels.int()\n",
        "          TP = ((labels[j])*(outputs[j])).sum()\n",
        "          TN = ((1-labels[j])*(1-outputs[j])).sum()\n",
        "          FP = ((1-labels[j])*(outputs[j])).sum()\n",
        "          FN = ((labels[j])*(1-outputs[j])).sum()\n",
        "\n",
        "          accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
        "          precision = TP/(TP+FP)\n",
        "          recall = TP/(TP+FN)\n",
        "          f1 = 2*(precision*recall)/(precision+recall)\n",
        "          '''\n",
        "          print(f\"Accuracy:{accuracy}\")\n",
        "          print(f\"Precision:{precision}\")\n",
        "          print(f\"Recall:{recall}\")\n",
        "          print(f\"F1 Score:{f1}\\n\")\n",
        "          '''\n",
        "          avgAcc+=accuracy.cpu().item()\n",
        "          avgPrec+=precision.cpu().item()\n",
        "          avgRec+=recall.cpu().item()\n",
        "          avgF1+=f1.cpu().item()\n",
        "\n",
        "    '''\n",
        "    #for visualization of the training data\n",
        "    for j in range(dataloader.batch_size):\n",
        "        plt.figure()\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.imshow(images[j].cpu().numpy().squeeze(), cmap='viridis')\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.imshow(labels[j].cpu().numpy().squeeze(), cmap='viridis')\n",
        "    '''\n",
        "    train_loss = running_loss/len(dataloader)\n",
        "    print(f\"Epoch {i+1}/{nrOfEpochs}\\nLoss:{train_loss}\")\n",
        "  return train_loss, avgAcc/len(dataloader), avgPrec/len(dataloader), avgRec/len(dataloader), avgF1/len(dataloader)\n",
        "\n",
        "\n",
        "#Evaluation\n",
        "def test(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  avgAcc=0.0\n",
        "  avgPrec=0.0\n",
        "  avgRec=0.0\n",
        "  avgF1=0.0\n",
        "  plotImage=0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in dataloader:\n",
        "      images, labels = images.to(device), labels.to(device);\n",
        "      outputs = model(images)\n",
        "      labels = Trans.center_crop(labels, [388,388])\n",
        "      loss = criterion(outputs,labels)\n",
        "      running_loss+=loss.item()\n",
        "      outputs = (outputs > 0.5).int()\n",
        "      #visualisation and evaluation\n",
        "      for j in range(dataloader.batch_size):\n",
        "        if(plotImage==6): #for picking single image to plot\n",
        "          plt.figure()\n",
        "          plt.subplot(2,2,1)\n",
        "          plt.title(f\"Image\")\n",
        "          plt.imshow(images[j].cpu().numpy().squeeze(), cmap='viridis')\n",
        "          plt.subplot(2,2,2)\n",
        "          plt.title(f\"Ground Truth\")\n",
        "          plt.imshow(labels[j].cpu().numpy().squeeze(), cmap='viridis')\n",
        "          plt.subplot(2,2,3)\n",
        "          plt.title(f\"Prediction\")\n",
        "          #outputs[j]=(outputs[j]>0.5).int()\n",
        "          plt.imshow(outputs[j].cpu().detach().numpy().squeeze(), cmap='viridis')\n",
        "          plt.subplot(2,2,4)\n",
        "          plt.title(f\"Difference\")\n",
        "          diff=(outputs[j]!=labels[j]).int()\n",
        "          plt.imshow(diff.cpu().detach().numpy().squeeze(),cmap='viridis')\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "        #Output image to binary values\n",
        "        #outputs[j] = (outputs > 0.5).float()\n",
        "        labels = labels.int()\n",
        "        TP = ((labels[j])*(outputs[j])).sum()\n",
        "        TN = ((1-labels[j])*(1-outputs[j])).sum()\n",
        "        FP = ((1-labels[j])*(outputs[j])).sum()\n",
        "        FN = ((labels[j])*(1-outputs[j])).sum()\n",
        "\n",
        "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        f1 = 2*(precision*recall)/(precision+recall)\n",
        "        '''\n",
        "        print(f\"Accuracy:{accuracy}\")\n",
        "        print(f\"Precision:{precision}\")\n",
        "        print(f\"Recall:{recall}\")\n",
        "        print(f\"F1 Score:{f1}\\n\")\n",
        "        '''\n",
        "        avgAcc+=accuracy.cpu().item()\n",
        "        avgPrec+=precision.cpu().item()\n",
        "        avgRec+=recall.cpu().item()\n",
        "        avgF1+=f1.cpu().item()\n",
        "\n",
        "        plotImage+=1\n",
        "\n",
        "  print(f\"Avg Accuracy:{avgAcc/len(dataloader)}\")\n",
        "  print(f\"Avg Precision:{avgPrec/len(dataloader)}\")\n",
        "  print(f\"Avg Recall:{avgRec/len(dataloader)}\")\n",
        "  print(f\"Avg F1 Score:{avgF1/len(dataloader)}\\n\")\n",
        "\n",
        "  test_loss = running_loss/len(dataloader)\n",
        "  print(f\"Test loss:{test_loss}\")\n",
        "  return test_loss, avgAcc/len(dataloader), avgPrec/len(dataloader), avgRec/len(dataloader), avgF1/len(dataloader)\n",
        "\n",
        "#10x number of epochs - DECIDES NR OF EPOCHS\n",
        "nrEpx10 = 1\n",
        "\n",
        "x = [0]*nrEpx10\n",
        "trainLoss = [0]*nrEpx10\n",
        "trainAcc = [0]*nrEpx10\n",
        "trainPrec = [0]*nrEpx10\n",
        "trainRec = [0]*nrEpx10\n",
        "trainF1 = [0]*nrEpx10\n",
        "testLoss = [0]*nrEpx10\n",
        "testAcc = [0]*nrEpx10\n",
        "testPrec = [0]*nrEpx10\n",
        "testRec = [0]*nrEpx10\n",
        "testF1 = [0]*nrEpx10\n",
        "\n",
        "\n",
        "#torch.load('unet_50ep_lr0001_mirr.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "#Running code:\n",
        "for i in range(nrEpx10):\n",
        "  print(f\"\\nLoop {i+1}\\n\")\n",
        "  x[i] = 10*(i+1)\n",
        "  trainLoss[i], trainAcc[i], trainPrec[i], trainRec[i], trainF1[i] = train(model, train_loader, criterion, optimizer, 10)\n",
        "  testLoss[i], testAcc[i], testPrec[i], testRec[i], testF1[i] = test(model, test_loader, criterion)\n",
        "  #update frame\n",
        "\n",
        "#Plot acc, prec, rec, f1 - add \"-o\" for dots\n",
        "fig = plt.figure(dpi=1200)\n",
        "#plt.title('Metrics over Epochs')\n",
        "plt.subplot(2,3,1)\n",
        "#plt.title(f\"Loss\")\n",
        "plt.plot(x, trainLoss, label='Train Loss')\n",
        "plt.plot(x, testLoss, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.subplot(2,3,2)\n",
        "#plt.title(f\"Accuracy\")\n",
        "plt.plot(x, trainAcc, label='Train Accuracy')\n",
        "plt.plot(x, testAcc, label='Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.subplot(2,3,3)\n",
        "#plt.title(f\"Precision\")\n",
        "plt.plot(x, trainPrec, label='Train Precision')\n",
        "plt.plot(x, testPrec, label='Test Precision')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Precision')\n",
        "plt.subplot(2,3,4)\n",
        "#plt.title(f\"Recall\")\n",
        "plt.plot(x, trainRec, label='Train Recall')\n",
        "plt.plot(x, testRec, label='Test Recall')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Recall')\n",
        "plt.subplot(2,3,5)\n",
        "#plt.title(f\"F1\")\n",
        "#plt.plot(x, trainF1, \"-o\", label='Train F1 Score')\n",
        "#plt.plot(x, testF1, \"-o\", label='Test F1 Score')\n",
        "plt.plot(x, trainF1 , label='Train')\n",
        "plt.plot(x, testF1, label='Test')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1')\n",
        "#plt.legend()\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Save model - change name to: unet _ nr of epochs _ learning rate 0.xxx as lrxxx _ which dataset\n",
        "#torch.save(model.state_dict(), 'unet_5000ep_lr0001_mirr_transf_SGD.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFWAnMB4qKWa",
        "outputId": "f5ac864f-8cbb-4c63-f8d0-e3e2f4067630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loop 1\n",
            "\n",
            "Epoch 1/10\n",
            "Loss:0.7191595176855723\n",
            "Epoch 2/10\n",
            "Loss:0.7093304673830668\n"
          ]
        }
      ]
    }
  ]
}